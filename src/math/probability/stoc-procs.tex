\documentclass[a4paper, 12pt, fleqn]{article}

\usepackage{amsmath,amsthm,amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{array}
\usepackage{comment}
\usepackage{booktabs}
\usepackage[bookmarksnumbered=true,hypertexnames=false]{hyperref}
%\usepackage{algorithm, algpseudocode}
\usepackage[capitalize,sort]{cleveref}

%\def\colorscheme{dark}
\input{colorscheme.tex}
\hypersetup{colorlinks,linkcolor=textRed,citecolor=textRed,urlcolor=textBlue}
\input{macros.tex}

\DeclareMathOperator{\boolone}{\mathbf{1}}
\DeclareMathOperator{\MGF}{MGF}
\DeclareMathOperator{\Poisson}{\opname{Poisson}}
\DeclareMathOperator{\Expo}{\opname{Expo}}

\author{Eklavya Sharma}
\date{\empty}

\title{Stochastic Processes}

\begin{document}

\maketitle
\setlength{\parskip}{0.2em}

\begin{definition}[Stochastic Process]
Let $\Tcal \subseteq \mathbb{R}$. For any $t \in \Tcal$,
let $X_t$ (or $X(t)$) be a random variable with support $D$.
Then $X \defeq \{X_t: t \in \Tcal\}$ is called a
\emph{stochastic process} on state-space $D$ and time $\Tcal$.
Usually, $\Tcal$ is either $\mathbb{Z}_{\ge 0}$ (discrete-time)
or $\mathbb{R}_{\ge 0}$ (continuous-time).
\end{definition}

\section{Discrete-Time Markov Chains}

\begin{definition}[Markov Chain]
Let $X \defeq [X_0, X_1, \ldots]$ be a stochastic process
on state-space $D$ and time $\mathbb{Z}_{\ge 0}$.
$X$ is called a discrete-time markov chain if
$\Pr(X_{t+1}=d \mid X_t, X_{t-1}, \ldots, X_0) = \Pr(X_{t+1}=d \mid X_t)$.
If $\Pr(X_{t+1}=v \mid X_t=u) = \Pr(X_1=v \mid X_0=u)$ for all $t, u, v$,
then $X$ is called \emph{time-homogeneous}.
\end{definition}

\begin{definition}[Transition function]
Let $X$ be a markov chain on state space $D$.
Define $P^{(k)}: D \times D \mapsto [0, 1]$ as $P^{(k)}(i, j) = \Pr(X_k = j \mid X_0 = i)$.
Then $P^{(k)}$ is called the $k$-step transition function of $X$.
For $k = 1$, we simply write $P$ instead of $P^{(1)}$.
For a finite state space, we can represent $P$ as a matrix.
\end{definition}

\begin{lemma}[Chapman-Kolmogorov Equation]
$P^{(m+n)}(i, j) = \sum_{k}P^{(m)}(i, k)P^{(n)}(k, j)$.
\end{lemma}

\subsection{Classification of States, Recurrence, Limiting Probabilities}

\begin{definition}
Let $f_{i,j} \defeq \Pr\left(\bigvee_{t \ge 1} (X_t = j) \Bigm| X_0 = i\right)$.
Then $f_{i,j}$ is called the \emph{eventual transition probability} from $i$ to $j$.
If $i = j$, then we write $f_{i,i}$ as $f_i$, and call it
the \emph{recurrence probability} of state $i$.
\end{definition}

\begin{definition}
For a state $i$, let $N_i$ be the random variable that counts the number of times
we are in state $i$, i.e., $N_i \defeq \sum_{t=0}^{\infty}\boolone(X_t = i)$.
Then $N_i$ is called the visit-count of $i$.
\end{definition}

\begin{definition}
\label{defn:recurrence}
A state $i$ of a markov chain is \emph{recurrent} iff (the following are equivalent):
\begin{itemize}
\item the recurrence probability ($f_i$) of $i$ is 1.
\item $i$ is visited infinitely often, i.e., $\Pr(N_i=\infty \mid X_0 = i) = 1$.
\item $i$ is visited infinitely often in expectation, i.e., $\E(N_i \mid X_0 = i) = \infty$.
\end{itemize}
A non-recurrent state is called a \emph{transient} state.
\end{definition}

\begin{lemma}
\label{thm:recurrence}
$\Pr(N_i = k \mid X_0 = i) = f_i^{k-1}(1-f_i)$.
\end{lemma}

\begin{lemma}
$\E(N_i \mid X_0 = i) = 1/(1-f_i) = \sum_{t=0}^{\infty}P^{(t)}(i,i)$.
\end{lemma}

\begin{definition}
State $j$ is \emph{accessible} from state $i$ if $P^{(t)}(i, j) > 0$ for some $t$.
States $i$ and $j$ \emph{communicate} (denoted as $i \leftrightarrow j$)
if $i$ and $j$ are both accessible from each other.
\end{definition}

\begin{lemma}
Accessibility is reflexive and transitive.
Communication is an equivalence relation.
The equivalence classes of communicability are called \emph{state classes}.
A markov chain is \emph{irreducible} if it has just one state class.
\end{lemma}

\begin{definition}
Let $T_i$ be the time when a markov chain moves to state $i$, i.e.,
$T_i \defeq \min_{t \ge 1} (X_t = i)$.
When conditioned on $X_0 = i$, $T_i$ is called the \emph{recurrence time} of $i$.
State $i$ is called \emph{positive recurrent} if $\E(T_i \mid X_0=i)$ is finite,
otherwise it is \emph{null recurrent}.
\end{definition}

\begin{lemma}
Recurrence and positive recurrence are class properties, i.e.,
they are same for all states in a class.
\end{lemma}

\begin{lemma}
In a finite-state markov chain, all recurrent states are positive recurrent,
and there is at least one recurrent state.
\end{lemma}

\begin{definition}[Periodicity]
For a state $i$, its period is defined as $\gcd(\{t: \Pr(T_i=t \mid X_0=i) > 0\})$.
A state is \emph{aperiodic} if its period is 1.
\end{definition}

\begin{lemma}
Periodicity is a class property.
\end{lemma}

\begin{definition}[Ergodicity]
A state is ergodic if it is positive recurrent and aperiodic.
A markov chain is ergodic if all its states are ergodic.
\end{definition}

\begin{lemma}
In an irreducible ergodic markov chain, for every state $j$,
$\lim_{t \to \infty} P^{(t)}(j, i) = \pi_i$ for a unique real number $\pi_i$.
$\pi_i$ is called the \emph{limiting probability} of state $i$.
Furthermore, $\pi_i$ is the unique solution to
this system of equations: $\pi_i = \sum_{j} \pi_j P(j, i)$ for all $i$
($\pi = P^T\pi$ in matrix form) and $\sum_i \pi_i = 1$.
\end{lemma}

\begin{lemma}
In an irreducible ergodic markov chain, $\E(T_i \mid X_0=i) = 1/\pi_i$.
\end{lemma}
\begin{corollary}
A state $i$ is null recurrent iff $\pi_i = 0$.
\end{corollary}

\begin{theorem}
If the transition function of markov chain $X$ is doubly-stochastic
(i.e., each row and each column sums to 1), then the limiting probability
of each state is $1/n$, where $n$ is the number of states.
\end{theorem}

\subsection{Time-Reversibility}

\begin{definition}
For an irreducible ergodic markov chain $X$ with limiting probabilities $\pi$.
Let $Y$ be a markov chain whose transition function is $Q(i, j) = P(j, i)(\pi_j/\pi_i)$.
Then $Y$ is called the \emph{time-reversed} markov chain of $X$.
$X$ is called \emph{time-reversible} if $Q = P$.
\end{definition}

\begin{theorem}
Let $X$ be a time-reversible markov chain with limiting probabilities $\pi$.
Then $\pi$ is the unique solution to this system of equations:
$x_jP(j, i) = x_iP(i, j)$ for all states $i$ and $j$, and $\sum_i x_i = 1$.
\end{theorem}

\begin{theorem}
If the transition function of markov chain $X$ is symmetric, then $X$ is time-reversible.
\end{theorem}

\subsection{Simple Random Walk}

Let $X$ be a TH MC on state space $S = I \cap \mathbb{Z}$,
where $I$ is an interval of $\mathbb{R}$ and
\[ \Pr(X_1 = j \mid X_0 = i) = \begin{cases}
p & \textrm{ if } j = i+1
\\ 1-p & \textrm{ if } j = i-1
\\ 0 & \textrm{ if } j \neq i
\end{cases}. \]

\begin{lemma}[Martingale property]
\label{thm:rand-walk:martin}
Let $\forall t \ge 1$. If $p = 1/2$, then $\E(X_t \mid X_{t-1}) = X_{t-1}$.
Otherwise, for $r \defeq (1-p)/p$, $\E(r^{X_t} \mid X_{t-1}) = r^{X_{t-1}}$.
\end{lemma}

\begin{lemma}
Let $I = [0, b]$, $T \defeq \min_{t \ge 0} (X_t \in \{0, b\})$, and $r \defeq (1-p)/p$.
Then $\forall i \in S$,
\begin{align*}
p_i \defeq \Pr(X_T = b \mid X_0 = i) &= \begin{cases}
\displaystyle \frac{i}{b} & \textrm{ if } p = 1/2
\\ \displaystyle \frac{r^i-1}{r^b-1} & \textrm{ if } p \neq 1/2
\end{cases}
\\ \mu_i \defeq \E(T \mid X_0 = i) &= \begin{cases}
i(b-i) & \textrm{ if } p = 1/2
\\ \displaystyle \frac{1}{1-2p}\left(i - b\frac{r^i-1}{r^b-1}\right) & \textrm{ if } p \neq 1/2
\end{cases}
\end{align*}
\end{lemma}
\begin{proof}[Proof sketch]
$p_0 = 0$, $p_b = 1$, and $p_i \defeq pp_{i+1} + (1-p)p_{i-1}$ $\forall i \in [b-1]$.
Rearrange to get $p(p_{i+1} - p_i) = (1-p)(p_i - p_{i-1})$ $\forall i \in [b-1]$.
Let $d_i \defeq p_i - p_{i-1}$ $\forall i \in [b]$. Then
\[ \forall i \in [b],\quad \frac{d_i}{d_1} = \prod_{j=1}^{i-1} \frac{d_{j+1}}{d_j}
= \prod_{j=1}^{i-1} r = r^{i-1}, \]
\[ \forall i \in S,\quad
p_i = \sum_{j=1}^i d_j = \sum_{j=1}^i d_1r^{j-1} = d_1 \times \begin{cases}
i & \textrm{ if } p = 1/2 \\ \frac{r^i-1}{r-1} & \textrm{ if } p \neq 1/2
\end{cases}. \]
Since $p_b = 1$, we get $d_1 = 1/b$ if $p = 1/2$ and $d_1 = (r-1)/(r^b-1)$ otherwise.

$\mu_0 = \mu_b = 0$, and $\mu_i = 1 + p\mu_{i+1} + (1-p)\mu_{i-1}$ $\forall i \in [b-1]$.
Rearrange to get $p(\mu_{i+1}-\mu_i) = (1-p)(\mu_i - \mu_{i-1}) - 1$ $\forall i \in [b-1]$.
Let $\nu_i \defeq \mu_i - \mu_{i-1}$ $\forall i \in [b]$.
Then $\forall i \in [b-1]$, $\nu_{i+1} = r\nu_i - 1/p$.

\textbf{Case 1}: $p = 1/2$
\[ \forall i \in [b],\quad \nu_i - \nu_1 = \sum_{j=1}^{i-1} (\nu_{j+1} - \nu_j) = -2(i-1). \]
\[ \forall i \in S,\quad \mu_i = \sum_{j=1}^i \nu_j = \sum_{j=1}^i (\nu_1 - 2(j-1))
= i(\nu_1 - (i - 1)). \]
Since $\mu_b = 0$, we get $\nu_1 = b-1$, and so $\mu_i = i(b-i)$.

\textbf{Case 2}: $p \neq 1/2$
\[ \nu_{i+1} = r\nu_i - \frac{1}{p} = r\nu_i - \frac{1}{p}\left(\frac{r}{r-1} - \frac{1}{r-1}\right)
\implies \nu_{i+1} - \frac{1}{p(r-1)} = r\left(\nu_i - \frac{1}{p(r-1)}\right). \]
$p(r-1) = 1-2p$. Hence,
\[ \forall i \in [b],\quad \frac{\nu_i - \frac{1}{1-2p}}{\nu_1 - \frac{1}{1-2p}}
= \prod_{j=1}^{i-1} \frac{\nu_{j+1} - \frac{1}{1-2p}}{\nu_j - \frac{1}{1-2p}}
= r^{i-1}. \]
\begin{align*}
\forall i \in S,\quad \mu_i &= \sum_{j=1}^i \nu_i
= \sum_{j=1}^i \left(\frac{1}{1-2p} + r^{j-1}\left(\nu_1 - \frac{1}{1-2p}\right)\right).
\\ &= \frac{i}{1-2p} + \left(\nu_1 - \frac{1}{1-2p}\right)\frac{r^i-1}{r-1}.
\end{align*}
Set $\mu_b = 0$ to get the answer.
\end{proof}

\begin{lemma}[\href{https://en.wikipedia.org/wiki/Catalan_number}{Catalan number}]
\label{thm:bal-paren}
The number of balanced parentheses strings of length $2n$ is
$\binom{2n}{n}\frac{1}{n+1}$.
\end{lemma}

\begin{lemma}
\label{thm:catalan-stirling}
For all $n \ge 1$,
\[ \binom{2n}{n}\frac{\sqrt{n}}{4^n} \in \left[\frac{2\sqrt{\pi}}{e^2}, \frac{e}{\sqrt{2}\pi}\right]. \]
\end{lemma}
\begin{proof}
Use Stirling's approximation: $n!e^n/(n^n\sqrt{n}) \in [\sqrt{2\pi}, e]$.
\end{proof}

\begin{lemma}
Let $S = \mathbb{Z}$. If $p \neq 1/2$, then every state is transient.
Otherwise, every state is null recurrent.
\end{lemma}
\begin{proof}
All states communicate, so all states belong to the same class.
Recurrence and positive recurrence are class properties.

Using \cref{defn:recurrence}, we get that state 0 is recurrent iff
$\E(N_0 \mid X_0=0)$, where $N_0$ is the number of times we visit state 0.
\begin{align*}
\E(N_0 \mid X_0=0) &= \sum_{t=0}^{\infty} P^{(t)}(0,0)  \tag{by \cref{thm:recurrence}}
\\ &= \sum_{j=0}^{\infty} 2\binom{2j}{j}p^j(1-p)^j  \tag{by \cref{thm:bal-paren}}
\\ &\le \frac{\sqrt{2}e}{\pi} \sum_{j=0}^{\infty} \frac{(4p(1-p))^j}{\sqrt{j}}.
    \tag{by \cref{thm:catalan-stirling}}
\end{align*}
$4p(1-p) \le 1$ and $p = 1/2 \iff 4p(1-p) = 1$.
Hence, the series is convergent iff $p \neq 1/2$.
Hence, state 0 is recurrent iff $p = 1/2$.

Let $p = 1/2$. Then $T \defeq \min_{t \ge 0} (X_t = 0)$
and $T' \defeq \min_{t \ge 1} (X_t = 0)$.
Then state 0 is null recurrent iff $\E(T' \mid X_0=0) = \infty$.
Since state 0 is recurrent, $\Pr(T' = \infty \mid X_0=0) = 0$.
Hence, $0 = \Pr(T' = \infty \mid X_0=0) = \Pr(T = \infty \mid X_0=1)$.
\begin{align*}
& \E(T' \mid X_0=0) = 1 + \E(T \mid X_0=1)
\\ &= 1 + \sum_{j=0}^{\infty} (2j+1)\Pr(T=2j+1 \mid X_0=1)
    \tag{since $\Pr(T = \infty \mid X_0=1) = 0$}
\\ &= 1 + \sum_{j=0}^{\infty} \frac{2j+1}{j+1} \binom{2j}{j}\frac{1}{2^{2j+1}}
    \tag{by \cref{thm:bal-paren}}
\\ &\ge 1 + \frac{\sqrt{\pi}}{e^2} \sum_{j=0}^{\infty} \frac{1}{\sqrt{j}}
    \tag{by \cref{thm:catalan-stirling}}
\\ &= \infty.
\end{align*}
Hence, 0 is null recurrent.
\end{proof}

\begin{lemma}
Let $I = [0, \infty)$. Let $T \defeq \min_{t \ge 0} (X_t = 0)$. Then for $i > 0$,
\[ \mu_i \defeq \E(T \mid X_0 = i) = \begin{cases}
i/(1-2p) & \textrm{ if } p < 1/2
\\ \infty & \textrm{ if } p \ge 1/2
\end{cases}. \]
\[ p_i \defeq \Pr(T \neq \infty \mid X_0 = i) = \begin{cases}
1 & \textrm{ if } p \le 1/2
\\ r^i & \textrm{ if } p > 1/2
\end{cases}. \]
\end{lemma}
\begin{proof}[Proof sketch]
$\mu_i = i\mu_1$ and $\mu_1 = 1 + p\mu_2 = 1 + 2p\mu_1$.
Suppose $\mu_1 \neq \infty$.
If $p > 1/2$, then $\mu_1 = 1/(1-2p) < 0$, which is a contradiction.
If $p = 1/2$, then $\mu_1 = 1 + \mu_1$, which is a contradiction.
Hence, $\mu_1 = \infty$ when $p \ge 1/2$.

$p_i = p_1^i$ and $p_1 = (1-p) + pp_2$.
Solving these equations gives us $p_1 \in \{1, r\}$.
When $p \le 1/2$, we get $r \ge 1$, but $p_1 \in [0, 1]$ (since $p_1$ is a probability).
Hence, $p_i = 1$ when $p \le 1/2$.

Let $p < 1/2$. Then $\mu_1 \in \{\infty, 1/(1-2p)\}$.
We will show that $\mu_1 \neq \infty$.
Since $p_i = 1$ for all $i \ge 0$, we have $\Pr(T = \infty \mid X_0 = i) = 0$. Hence,
\begin{align*}
\mu_1 &= \E(T \mid X_0=1) = \sum_{j=0}^{\infty} (2j+1)\Pr(T=2j+1 \mid X_0=1)
\\ &= \sum_{j=0}^{\infty} \frac{2j+1}{j+1} \binom{2j}{j}p^j(1-p)^{j+1}
    \tag{by \cref{thm:bal-paren}}
\\ &\le 2(1-p)\frac{e}{\sqrt{2}\pi} \sum_{j=0}^{\infty} \frac{(4p(1-p))^j}{\sqrt{j}}.
    \tag{by \cref{thm:catalan-stirling}}
\end{align*}
This series is convergent, since $4p(1-p) < 1$ for $p < 1/2$.
Hence, $\mu_1 \neq \infty$, so $\mu_1 = 1/(1-2p)$.

Let $p > 1/2$.
Consider the random walk $Y$ on state space $\mathbb{Z}$ where $\forall t \ge 0$,
$\Pr(Y_{t+1} = j+1 \mid Y_t = j) = p$ and $\Pr(Y_{t+1} = j-1 \mid Y_t=j) = 1-p$.
Let $T' \defeq \min_{t \ge 0} (Y_t = 0)$.
Then $\Pr(T \neq \infty \mid X_0 = 1) = 1 \iff \Pr(T' \neq \infty \mid Y_0 = 1) = 1$.
$\Pr(T' \neq \infty \mid Y_0 = -1) = 1$, using the $p < 1/2$ case.
Hence, we return to state 0 in $Y$ with probability 1.
Hence, 0 is a recurrent state in $Y$, which is a contradiction.
Hence, $p_1 = \Pr(T \neq \infty \mid X_0 = 1) \neq 1$. Hence, $p_1 = r$.
\end{proof}

\section{Counting Process}

\begin{definition}[Counting Process]
Let $N$ be a stochastic process on state space $\mathbb{Z}_{\ge 0}$ and time $\mathbb{R}_{\ge 0}$.
Then $N$ is called a counting process if $N(0) = 0$ and $N(t)$ is monotone in $t$,
i.e., $t_1 < t_2 \implies N(t_1) \le N(t_2)$.
\end{definition}

\begin{definition}[Independent increments]
A counting process $N$ has \emph{independent increments} iff
for any two disjoint intervals $(u_1, v_1]$ and $(u_2, v_2]$ in $\mathbb{R}_{\ge 0}$,
the random variables $N(v_1) - N(u_1)$ and $N(v_2) - N(u_2)$ are independent.
\end{definition}

\begin{definition}[Stationary increments]
A counting process $N$ has \emph{stationary increments} iff for any $u \le v$,
the random variables $N(v) - N(u)$ and $N(v-u)$ have the same distribution.
\end{definition}

\begin{definition}[Arrival and interarrival times]
For a counting process $N$, for $i \in \mathbb{Z}_{\ge 0}$,
define the $i\Th$ arrival time $S_i \defeq \min_{t \ge 0} (N(t) = i)$.
For $i \in \mathbb{Z}_{\ge 1}$, define the $i\Th$ interarrival time $T_i \defeq S_i - S_{i-1}$.
\end{definition}

\begin{lemma}
For a counting process $N$ with arrival times $S$, $N(t) \ge n \iff S_n \le t$.
\end{lemma}

\begin{definition}[Stopping time]
\label{defn:stopping-time}
Let $X = [X_1, X_2, \ldots]$ be a sequence of random variables.
The random variable $N$ is called a stopping time for $X$ if for all $n \ge 0$,
(the following two definitions are equivalent):
\begin{itemize}
\item $N = n$ is independent of $X_{n+1}, X_{n+2}, \ldots$.
\item $N \le n$ is independent of $X_{n+1}, X_{n+2}, \ldots$.
\end{itemize}
\end{definition}

\begin{theorem}[Wald's identity]
Let $X = [X_1, X_2, \ldots]$ be a sequence of random variables where $\E(X_i) = \mu$ for all $i$.
Let $N$ be a stopping time for $X$. Then
\[ \E\left(\sum_{i=1}^N X_i\right) = \mu\E(N). \]
\end{theorem}
\begin{proof}[Proof sketch]
For all $i$, $N \ge i$ is independent of $X_i$, and
$\displaystyle \sum_{i=1}^N X_i = \sum_{i=1}^{\infty} X_i\boolone(N \ge i)$.
\end{proof}

\section{Poisson Process}

\begin{definition}[Poisson process]
A counting process $N$ is a Poisson process with rate function
$\lambda: \mathbb{R}_{\ge 0} \mapsto \mathbb{R}_{\ge 0}$ if
$N$ has independent increments and $N(t_2)-N(t_1) \sim \Poisson(\mu)$,
where $\mu \defeq \int_{t_1}^{t_2} \lambda(t) dt$.
$N$ is called \emph{homogeneous} if $\lambda(t) = \lambda(0)$ for all $t$,
otherwise it is called \emph{inhomogeneous}.
For a homogeneous process, we denote $\lambda(0)$ by $\lambda$.
\end{definition}

\begin{lemma}
A Poisson process $N$ is homogeneous iff it has stationary increments.
\end{lemma}

\begin{theorem}[Alternative definition of Poisson process]
A counting process $N$ is a Poisson process with continuous rate function $\lambda$ iff
$N$ has independent and stationary increments and
$\Pr(N(t+h)-N(t) = 1) = \lambda(t) h + o(h)$ and $\Pr(N(t+h)-N(t) \ge 2) = o(h)$.
\end{theorem}
\begin{proof}[Proof sketch for homogeneous]
Let $g(u,t) \defeq \MGF_u(N(t)) = \E(e^{uN(t)})$.
Show $g(u,t) = 1 + \lambda t(e^u-1) + o(t)$ straightforwardly.
Use calculus to show that $g(u,t) = \exp(e^{\lambda t}(e^u-1))$
(find derivative w.r.t $t$ by computing $\lim_{h \to 0} (g(u,t+h)-g(u,t))/h$; this gets rid of $o(h)$).
Conclude that $N(t) \sim \Poisson(\lambda t)$ since $g(u,t)$ is MGF of $\Poisson(\lambda t)$.
\end{proof}

\begin{lemma}
For a homogeneous Poisson process $N$,
\[ \Pr(N(s)=a \mid N(s+t)=a+b) = \binom{a+b}{a}\left(\frac{s}{s+t}\right)^a\left(\frac{t}{s+t}\right)^b. \]
\end{lemma}

\begin{theorem}
Let $N$ be a counting process. Then $N$ is a homogeneous Poisson process with rate $\lambda$
iff all interarrival times are independent and distributed $\Expo(\lambda)$.
\end{theorem}

\begin{theorem}[Decomposition theorem 1]
Let $K$ be a finite set, and let $\{N_i: i \in K\}$ be independent Poisson processes,
where $N_i$ has rate function $\lambda_i$. Let $N \defeq \sum_{i \in K}N_i$.
Then $N$ is a Poisson process with rate function $\sum_{i \in K} \lambda_i$.
\end{theorem}

\begin{theorem}[Decomposition theorem 2]
Let $N$ be a Poisson process with rate function $\lambda$. Let $K$ be a finite set (called set of labels).
Suppose the $j\Th$ event receives label $L_j \in K$, where $\Pr(L_j = i) = p_i(S_j)$
for some function $p_i: \mathbb{R}_{\ge 0} \mapsto \mathbb{R}_{\ge 0}$,
and $\{N, L_1, L_2, \ldots\}$ are independent.
For $i \in K$, let $N_i(t)$ be the number of events having label $i$, i.e,
$N_i(t) = \sum_{j=1}^{N(t)} \boolone(L_j = i)$.
Then $N_i$ is a Poisson process with rate function $p_i \lambda$.
Furthermore, all $N_i$ are independent and if all $p_i$ are constant, then
$N_i(t) \mid N(t) \sim \opname{Binom}(N(t), p_i)$.
\end{theorem}

\begin{lemma}
Let $N^{(1)}$ and $N^{(2)}$ be independent homogeneous Poisson processes
with rates $\lambda_1$ and $\lambda_2$. Then
\[ \Pr(S^{(1)}_n < S^{(2)}_m) = \sum_{i=n}^{n+m-1} \binom{n+m-1}{i}
    \frac{\lambda_1^i\lambda_2^{n+m-1-i}}{(\lambda_1 + \lambda_2)^{n+m-1}}. \]
\end{lemma}
\begin{proof}[Proof sketch]
Model as a continuous markov chain with state space $(n_1, n_2)$,
where $n_i$ is the number of events of $N^{(i)}$ that have occurred.
\end{proof}

\begin{theorem}[arrival times distributed as order statistics]
Let $X = [X_1, X_2, \ldots, X_n]$ be IID uniform variables over $[0, t]$.
Let $U = \opname{sorted}(X)$. Let $N$ be a homogeneous Poisson process.
Let $S_i$ be the $i\Th$ arrival time of $N$.
Then conditioned on $N(t) = n$, the distribution of $[S_1, \ldots, S_n]$ and $U$ are identical.
\end{theorem}

\begin{lemma}[Excess and Residual]
Let $N$ be a Poisson process with rate function $\lambda$.
Let $S_i$ be the $i\Th$ arrival time.
Let $Y(t) \defeq S_{N(t)+1}-t$ and $R(t) \defeq t - S_{N(t)}$.
Then $Y(t) > s \iff N(t+s)-N(t) = 0$ and $R(t) > r \iff N(t)-N(t-r) = 0$.
If $N$ is homogeneous, we get $Y(t) \sim \Expo(\lambda)$ and $R(t) \sim \Expo(\lambda)$.
\end{lemma}

\section{Continuous-Time Markov Chain}

\begin{definition}[CTMC]
Let $X \defeq \{X(t): t \in \mathbb{R}_{\ge 0}\}$ be a stochastic process
on discrete state-space $D$.
$X$ is called a continuous-time markov chain (CTMC) if
$\Pr(X(t+s) = d \mid \{X(u): 0 \le u \le s\}) = \Pr(X(t+s)=d \mid X(s))$
for all $s, t \in \mathbb{R}_{\ge 0}$.
If $\Pr(X(t+s)=v \mid X(s)=u) = \Pr(X(t)=v \mid X(0)=u)$ for all $u, v, s, t$,
then $X$ is called \emph{time-homogeneous} (TH) or \emph{stationary}.
\end{definition}

\begin{theorem}[Equiv defn of TH CTMC]
Let $X \defeq \{X(t): t \in \mathbb{R}_{\ge 0}\}$ be a stochastic process
on discrete state-space $D$. Let $Y(t) \defeq \{X(u): 0 \le u < t \}$.
Let $T_i^{(s)} \defeq \min_{t \ge 0} (X(t+s) \neq i)$.
Let $P^{(s)}_{i,j} \defeq \Pr(X(s+T_i^{(s)})=j \mid X(s)=i, Y(s))$.
$X$ is TH CTMC iff $(T_i^{(s)} \mid X(s)=i, Y(s)) \sim \Expo(\nu_i)$,
where $\nu_i$ is a constant that doesn't depend on $s$ or $Y(s)$,
and $P^{(s)}_{i,j}$ is a constant that doesn't depend on $s$ or $Y(s)$.

Since $T_i^{(s)}$ and $P^{(s)}_{i,j}$ don't depend on $s$, we simply write $T_i$ and $P_{i,j}$.
$T_i$ is called the \emph{transition time} out of state $i$,
$\nu_i$ is called the \emph{transition rate} out of state $i$,
and $P_{i,j}$ is the \emph{probability of transitioning} from state $i$ to state $j$.
\end{theorem}

Let $q_{i,j} \defeq \nu_iP_{i,j}$. Then $\nu_i = \sum_j q_{i,j}$.

\begin{theorem}[Chapman-Kolmogorov DiffEqs]
For a TH CTMC $X$, let $P_{i,j}(t) \defeq \Pr(X(t)=j \mid X(0)=i)$. Then
\begin{itemize}
\item Backward DiffEqs: $\displaystyle \frac{dP_{i,j}(t)}{dt} = \sum_{k \neq i} q_{i,k}P_{k,j}(t) - \nu_iP_{i,j}(t)$.
\item Forward DiffEqs: $\displaystyle \frac{dP_{i,j}(t)}{dt} = \sum_{k \neq j} P_{i,k}(t)q_{k,j} - P_{i,j}(t)\nu_j$.  \end{itemize}
Let $r_{i,j} \defeq \begin{cases}q_{i,j} & \textrm{ if } i \neq j \\ -\nu_i & \textrm{ if } i = j\end{cases}$.
Let the state space be $[n]$. Let $R$ be a matrix where $R[i,j] = r_{i,j}$.
Then CBKE becomes $P'(t) = RP(t)$ and CFKE becomes $P'(t) = P(t)R$.
\end{theorem}

\begin{lemma}
CKBE $P'(t) = RP(t)$ solves to $P(t) = e^{Rt}$,
where $e^{A} \defeq \sum_{i=0}^{\infty} A^i/i!$ for any square matrix $A$.
Suppose $R$ has $n$ eigenpairs $\{(\lambda_1, v_i): i \in [n]\}$.
Let $P$ be a square matrix whose $i\Th$ column is $v_i$,
and $D$ be a diagonal matrix whose $i\Th$ diagonal entry is $\lambda_i$.
Then $R = PDP^{-1}$, $e^{Rt} = Pe^{Dt}P^{-1}$,
and $e^{Dt} = \opname{diag}([e^{\lambda_1t}, \ldots, e^{\lambda_nt}])$.
\end{lemma}

\begin{lemma}
Let $X$ be a TH CTMC.
\begin{align*}
\lim_{h \to 0} \frac{1-P_{i,i}(h)}{h} &= \nu_i\quad\forall i
& \lim_{h \to 0} \frac{P_{i,j}(h)}{h} &= q_{i,j}\quad\forall i \neq j
\end{align*}
\end{lemma}

\begin{lemma}[Limiting probability]
\label{thm:ctmc:limprob}
In an irreducible positive-recurrent TH CTMC $X$, for every state $j$,
$\lim_{t \to \infty} P_{j,i}(t) = P_i$ for a unique real number $P_i$.
$P_i$ is called the \emph{limiting probability} of state $i$.
Furthermore, $P_i$ is the unique solution to $\sum_i P_i = 1$ and CK forward equations,
i.e., $P_i\nu_i = \sum_{j \neq i} P_jq_{j,i}$.
\end{lemma}

\begin{lemma}[Limiting probability of embedded chain]
Let $X$ be an irreducible positive-recurrent TH CTMC. Let $Y$ be the sequence of states visited by $X$.
Then $Y$ is a discrete MC. Let $P$ and $\pi$ be the limiting probabilities of $X$ and $Y$, respectively.
Then $P_i = (\pi_i/\nu_i)/(\sum_j \pi_j/\nu_j)$ and $\pi_i = P_i\nu_i/(\sum_j P_j\nu_j)$.
\end{lemma}

\begin{definition}
A CTMC is time-reversible iff the corresponding embedded discrete-time MC is time-reversible.
\end{definition}

\begin{lemma}[2-state]
For a CTMC on states $\{0, 1\}$, where $q_{0,1} = \lambda$ and $q_{1,0} = \mu$, we get
\[ P(t) = \frac{1}{\lambda + \mu}\left(
\begin{bmatrix}\mu & \lambda \\ \mu & \lambda\end{bmatrix}
+ e^{-(\mu+\lambda)t}\begin{bmatrix}\lambda & -\lambda \\ -\mu & \mu\end{bmatrix}
\right). \]
\end{lemma}

\subsection{Birth and Death Process}

\begin{definition}
A birth-and-death (B\&D) process is a TH CTMC $X$ on state space $\mathbb{Z}_{\ge 0}$ where
$q_{i,j} = 0$ if $j \not\in \{i-1, i+1\}$. Let $\lambda_i \defeq q_{i,i+1}$ for $i \ge 0$,
$\mu_i \defeq q_{i,i-1}$ for $i \ge 1$, $\mu_0 \defeq 0$.

$X(t)$ is called the \emph{population} at time $t$,
$\lambda_i$ is called the \emph{birth rate} at population $i$,
and $\mu_i$ is called the \emph{death rate} at population $i$.
\end{definition}

\begin{lemma}
Let $X$ be a B\&D process where $X(0)=n$.
Let $T_n$ be the time to reach state $n+1$, i.e., $T_n \defeq \min_{t \ge 0} (X(t) = n+1)$. Then
\begin{align*}
\E(T_n) &= \frac{1}{\lambda_n} + \frac{\mu_n}{\lambda_n}\E(T_{n-1})
= \frac{1}{\lambda_n}\sum_{i=0}^n \prod_{j=1}^i \frac{\mu_{n-j+1}}{\lambda_{n-j}}.
\\ \Var(T_n) &= \frac{1}{\lambda_n(\lambda_n + \mu_n)^2} + \frac{\mu_n}{\lambda_n}\Var(T_{i-1})
    + \frac{\mu_n}{\lambda_n + \mu_n}(\E(T_{n-1}) + \E(T_n))^2
\end{align*}
\end{lemma}
\begin{proof}[Proof sketch]
Let $I_i = \boolone(\textrm{next transition goes to state } i+1)$.
Let $X_i$ be the transition time out of state $i$.
Then $I_i \sim \opname{Bernouilli}(\lambda_i/(\mu_i + \lambda_i))$,
$X_i \sim \Expo(\lambda_i + \mu_i)$, and
\begin{align*}
\E(T_i \mid I_i) &= \E(X_i) + (1-I_i)(\E(T_{i-1}) + \E(T_i))),
\\ \Var(T_i \mid I_i) &= \Var(X_i) + (1-I_i)(\Var(T_{i-1}) + \Var(T_i)).
\qedhere \end{align*}
\end{proof}

CKBE for B\&D:
\[ \frac{dP_{i,j}(t)}{dt} = \mu_iP_{i-1,j}(t) + \lambda_iP_{i+1,j}(t) - (\lambda_i + \mu_i)P_{i,j}(t). \]

CKFE for B\&D:
\[ \frac{dP_{i,j}(t)}{dt} = \mu_{j+1}P_{i,j+1}(t) + \lambda_{j-1}P_{i,j-1}(t) - (\lambda_j + \mu_j)P_{i,j}(t). \]

\begin{theorem}[Limiting Probabilities]
Let $X$ be an irreducible B\&D process on state space $D \subseteq \mathbb{Z}_{\ge 0}$ where $0 \in D$.
For $n \in D$, let $\alpha_n \defeq \prod_{i=1}^n \frac{\lambda_{i-1}}{\mu_i}$.
If $\sum_{i \in D} \alpha_i$ is finite, then
$P_i = \alpha_iP_0$, and $P_0 = 1/\sum_{i \in D} \alpha_i$.
\end{theorem}
\begin{proof}[Proof sketch]
Use \cref{thm:ctmc:limprob} and add adjacent equations.
\end{proof}

\section{Renewal Theory}

\begin{definition}
Let $[X_1, X_2, \ldots]$ be a sequence of IID non-negative randvars, called \emph{interarrival times},
such that $\Pr(X_1 = 0) < 1$ and $\Pr(X_1 = \infty) = 0$.
Let $S_n \defeq \sum_{i=1}^n X_i$ (called \emph{arrival times}). Let $N(t) \defeq \max_n (S_n \le t)$.
Then $N$ is called a \emph{renewal process} (note that it is a counting process).

We let $F$ and $f$ denote the CDF and PDF/PMF of $X_1$, respectively.
We let $F^{(n)}$ and $f^{(n)}$ denote the CDF and PDF/PMF of $S_n$, respectively.

Let $R_i$ be the \emph{reward} obtained at time $X_i$ for all $i \ge 1$,
where all $R_i$ are independent. Let $R(t) \defeq \sum_{i=1}^{N(t)} R_i$.
Then $R$ is called a \emph{renewal reward process}.
\end{definition}

\begin{lemma}
For all $t \ge 0$, $\Pr(N(t) = \infty) = 0$. $\Pr(\lim_{t \to \infty} N(t) = \infty) = 1$.
\end{lemma}
\begin{proof}
Let $\mu \defeq \E(X_1)$. $\mu > 0$ since $\Pr(X_n = 0) < 1$.
\[ \Pr\left(\lim_{t \to \infty} \frac{S_n}{n} = \mu\right) = 1.  \tag{strong law of large numbers} \]
\[ N(t) = \infty \iff (\forall n, S_n \le t) \implies \lim_{t \to \infty} \frac{S_n}{n} = 0. \]
$\Pr(N(\infty) = \infty) = 1$ since $\Pr(X_1 = \infty) = 0$.
\end{proof}

\begin{definition}
For a renewal process $N$, let $m_N(t) \defeq \E(N(t))$.
Then $m_N$ is called the \emph{mean-value function} of $N$.
(If $N$ is clear from context, we will write $m$ instead of $m_N$.)
\end{definition}

\begin{lemma}
$m(t) = \sum_{n=1}^{\infty} \Pr(S_n \le t) = \sum_{n=1}^{\infty} F^{(n)}(t)$.
\end{lemma}

\begin{theorem}
$m$ uniquely characterizes $F$.
\end{theorem}

\begin{lemma}
$m(t)$ is finite for all $t$.
\end{lemma}

\begin{theorem}[Renewal equation]
When interarrival times are continuous randvars,
\[ m(t) = F(t) + \int_0^t m(t-x)f(x)dx. \]
\end{theorem}
\begin{proof}[Proof sketch]
Let $N'(t) \defeq \max_n \left( \sum_{i=2}^{n+1} X_i \le t \right)$.
Then $N$ and $N'$ are identically distributed and
\[ N(t) = \begin{cases} 1 + N'(t-X_1) & \textrm{ if } X_1 \le t
\\ 0 & \textrm{ if } X_1 > t \end{cases}. \]
Finally, $m(t) = \E(\E(N(t) \mid X_1))$.
\end{proof}

\begin{corollary}
Let $N$ be a renewal process where interarrival times are distributed $\opname{Uniform}(0, 1)$.
Then for $0 \le t \le 1$, $m(t) = e^t - 1$.
\end{corollary}

\begin{theorem}[Limit theorems]
For a renewal process $N$ with $\mu \defeq \E(X_1)$,
\begin{align*}
& \Pr\left(\lim_{t \to \infty} \frac{N(t)}{t} = \frac{1}{\mu}\right) = 1.
&& \lim_{t \to \infty} \frac{m(t)}{t} = \frac{1}{\mu}.
\end{align*}
\end{theorem}

\begin{theorem}[Limit theorems for rewards]
For a renewal process $N$ with rewards $\{R_i: i \in \mathbb{Z}_{\ge 1}\}$,
let $\alpha \defeq \E(R_1)$ and $\mu \defeq \E(X_1)$. Then
\begin{align*}
& \Pr\left(\lim_{t \to \infty} \frac{R(t)}{t} = \frac{\alpha}{\mu}\right) = 1.
&& \lim_{t \to \infty} \frac{\E(R(t))}{t} = \frac{\alpha}{\mu}.
\end{align*}
\end{theorem}

\begin{theorem}[Central limit theorem for renewals]
For a renewal process $N$ with $\mu \defeq \E(X_1)$ and $\sigma^2 \defeq \Var(X_1)$,
the random variable
\[ \lim_{t \to \infty} \frac{N(t) - t/\mu}{\sqrt{t\sigma^2/\mu^3}} \]
tends to the standard normal distribution.
\end{theorem}

\begin{lemma}[Stopping time]
Let $X = [X_1, X_2, \ldots]$ be the sequence of interarrival times for renewal process $N$.
Then $N(t)+1$ is a stopping time for $X$.
\end{lemma}
\begin{proof}[Proof sketch]
$N(t)+1 \le n \iff S_n > t$.
\end{proof}

\begin{definition}
For a renewal process $N$ with arrival times $S_1, S_2, \ldots$:
\begin{itemize}
\item Let $Y(t) \defeq S_{N(t)+1}-t$. $Y(t)$ is called the \emph{excess} at time $t$.
\item Let $L(t) \defeq t - S_{N(t)}$. $L(t)$ is called the \emph{remaining life} at time $t$.
\end{itemize}
\end{definition}

\begin{lemma}
Let $N$ be a renewal process with interarrival times $X = [X_1, X_2, \ldots]$.
Then $\E(S_{N(t)+1}) = t + \E(Y(t)) = \E(X_1)(m(t)+1)$.
\end{lemma}
\begin{proof}
$N(t)+1$ is a stopping time for $X$.
\end{proof}

\end{document}
