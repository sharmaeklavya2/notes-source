\documentclass[a4paper, 12pt, fleqn]{article}

\usepackage{amsmath,amsthm,amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{array}
\usepackage{comment}
\usepackage{booktabs}
\usepackage[bookmarksnumbered=true,hypertexnames=false]{hyperref}
%\usepackage{algorithm, algpseudocode}
\usepackage[capitalize,sort]{cleveref}

%\def\colorscheme{dark}
\input{colorscheme.tex}
\hypersetup{colorlinks,linkcolor=textRed,citecolor=textRed,urlcolor=textBlue}
\input{macros.tex}

\DeclareMathOperator{\boolone}{\mathbf{1}}
\DeclareMathOperator{\MGF}{MGF}
\DeclareMathOperator{\Poisson}{\opname{Poisson}}
\DeclareMathOperator{\Expo}{\opname{Expo}}

\author{Eklavya Sharma}
\date{\empty}

\title{Stochastic Processes}

\begin{document}

\maketitle
\setlength{\parskip}{0.2em}

\begin{definition}[Stochastic Process]
Let $\Tcal \subseteq \mathbb{R}$. For any $t \in \Tcal$,
let $X_t$ (or $X(t)$) be a random variable with support $D$.
Then $X \defeq \{X_t: t \in \Tcal\}$ is called a
\emph{stochastic process} on state-space $D$ and time $\Tcal$.
Usually, $\Tcal$ is either $\mathbb{Z}_{\ge 0}$ (discrete-time)
or $\mathbb{R}_{\ge 0}$ (continuous-time).
\end{definition}

\section{Discrete-Time Markov Chains}

\begin{definition}[Markov Chain]
Let $X \defeq [X_0, X_1, \ldots]$ be a stochastic process
on state-space $D$ and time $\mathbb{Z}_{\ge 0}$.
$X$ is called a discrete-time markov chain if
$\Pr(X_{t+1}=d \mid X_t, X_{t-1}, \ldots, X_0) = \Pr(X_{t+1}=d \mid X_t)$.
If $\Pr(X_{t+1}=d \mid X_t) = \Pr(X_1=d \mid X_0)$, then $X$ is called \emph{time-homogeneous}.
\end{definition}

\begin{definition}[Transition function]
Let $X$ be a markov chain on state space $D$.
Define $P^{(k)}: D \times D \mapsto [0, 1]$ as $P^{(k)}(i, j) = \Pr(X_k = j \mid X_0 = i)$.
Then $P^{(k)}$ is called the $k$-step transition function of $X$.
For $k = 1$, we simply write $P$ instead of $P^{(1)}$.
For a finite state space, we can represent $P$ as a matrix.
\end{definition}

\begin{lemma}[Chapman-Kolmogorov Equation]
$P^{(m+n)}(i, j) = \sum_{k}P^{(m)}(i, k)P^{(n)}(k, j)$.
\end{lemma}

\subsection{Classification of States, Recurrence, Limiting Probabilities}

\begin{definition}
Let $f_{i,j} \defeq \Pr\left(\bigvee_{t \ge 1} (X_t = j) \Bigm| X_0 = i\right)$.
Then $f_{i,j}$ is called the \emph{eventual transition probability} from $i$ to $j$.
If $i = j$, then we write $f_{i,i}$ as $f_i$, and call it
the \emph{recurrence probability} of state $i$.
\end{definition}

\begin{definition}
For a state $i$, let $N_i$ be the random variable that counts the number of times
we are in state $i$, i.e., $N_i \defeq \sum_{t=0}^{\infty}\boolone(X_t = i)$.
Then $N_i$ is called the visit-count of $i$.
\end{definition}

\begin{definition}
A state $i$ of a markov chain is \emph{recurrent} iff (the following are equivalent):
\begin{itemize}
\item the recurrence probability ($f_i$) of $i$ is 1.
\item $i$ is visited infinitely often, i.e., $\Pr(N_i=\infty \mid X_0 = i) = 1$.
\item $i$ is visited infinitely often in expectation, i.e., $\E(N_i \mid X_0 = i) = \infty$.
\end{itemize}
A non-recurrent state is called a \emph{transient} state.
\end{definition}

\begin{lemma}
$\Pr(N_i = k \mid X_0 = i) = f_i^{k-1}(1-f_i)$.
\end{lemma}

\begin{lemma}
$\E(N_i \mid X_0 = i) = 1/(1-f_i) = \sum_{t=0}^{\infty}P^{(t)}(i,i)$.
\end{lemma}

\begin{definition}
State $j$ is \emph{accessible} from state $i$ if $P^{(t)}(i, j) > 0$ for some $t$.
States $i$ and $j$ \emph{communicate} (denoted as $i \leftrightarrow j$)
if $i$ and $j$ are both accessible from each other.
\end{definition}

\begin{lemma}
Accessibility is reflexive and transitive.
Communication is an equivalence relation.
The equivalence classes of communicability are called \emph{state classes}.
A markov chain is \emph{irreducible} if it has just one state class.
\end{lemma}

\begin{definition}
Let $T_i$ be the time when a markov chain moves to state $i$, i.e.,
$T_i \defeq \min_{t \ge 1} (X_t = i)$.
When conditioned on $X_0 = i$, $T_i$ is called the \emph{recurrence time} of $i$.
State $i$ is called \emph{positive recurrent} if $\E(T_i \mid X_0=i)$ is finite,
otherwise it is \emph{null recurrent}.
\end{definition}

\begin{lemma}
Recurrence and positive recurrence are class properties, i.e.,
they are same for all states in a class.
\end{lemma}

\begin{lemma}
In a finite-state markov chain, all recurrent states are positive recurrent,
and there is at least one recurrent state.
\end{lemma}

\begin{definition}[Periodicity]
For a state $i$, its period is defined as $\gcd(\{t: \Pr(T_i=t \mid X_0=i) > 0\})$.
A state is \emph{aperiodic} if its period is 1.
\end{definition}

\begin{lemma}
Periodicity is a class property.
\end{lemma}

\begin{definition}[Ergodicity]
A state is ergodic if it is positive recurrent and aperiodic.
A markov chain is ergodic if all its states are ergodic.
\end{definition}

\begin{lemma}
In an irreducible ergodic markov chain, for every state $j$,
$\lim_{t \to \infty} P^{(t)}(j, i) = \pi_i$ for a unique real number $\pi_i$.
$\pi_i$ is called the \emph{limiting probability} of state $i$.
Furthermore, $\pi_i$ is the unique solution to
this system of equations: $\pi_i = \sum_{j} \pi_j P(j, i)$ for all $i$
($\pi = P^T\pi$ in matrix form) and $\sum_i \pi_i = 1$.
\end{lemma}

\begin{lemma}
In an irreducible ergodic markov chain, $\E(T_i \mid X_0=i) = 1/\pi_i$.
\end{lemma}
\begin{corollary}
A state $i$ is null recurrent iff $\pi_i = 0$.
\end{corollary}

\begin{theorem}
If the transition function of markov chain $X$ is doubly-stochastic
(i.e., each row and each column sums to 1), then the limiting probability
of each state is $1/n$, where $n$ is the number of states.
\end{theorem}

\subsection{Time-Reversibility}

\begin{definition}
For an irreducible ergodic markov chain $X$ with limiting probabilities $\pi$.
Let $Y$ be a markov chain whose transition function is $Q(i, j) = P(j, i)(\pi_j/\pi_i)$.
Then $Y$ is called the \emph{time-reversed} markov chain of $X$.
$X$ is called \emph{time-reversible} if $Q = P$.
\end{definition}

\begin{theorem}
Let $X$ be a time-reversible markov chain with limiting probabilities $\pi$.
Then $\pi$ is the unique solution to this system of equations:
$x_jP(j, i) = x_iP(i, j)$ for all states $i$ and $j$, and $\sum_i x_i = 1$.
\end{theorem}

\begin{theorem}
If the transition function of markov chain $X$ is symmetric, then $X$ is time-reversible.
\end{theorem}

\section{Counting Process}

\begin{definition}[Counting Process]
Let $N$ be a stochastic process on state space $\mathbb{Z}_{\ge 0}$ and time $\mathbb{R}_{\ge 0}$.
Then $N$ is called a counting process if $N(0) = 0$ and $N(t)$ is monotone in $t$,
i.e., $t_1 < t_2 \implies N(t_1) \le N(t_2)$.
\end{definition}

\begin{definition}[Independent increments]
A counting process $N$ has \emph{independent increments} iff
for any two disjoint intervals $(u_1, v_1]$ and $(u_2, v_2]$ in $\mathbb{R}_{\ge 0}$,
the random variables $N(v_1) - N(u_1)$ and $N(v_2) - N(u_2)$ are independent.
\end{definition}

\begin{definition}[Stationary increments]
A counting process $N$ has \emph{stationary increments} iff for any $u \le v$,
the random variables $N(v) - N(u)$ and $N(v-u)$ have the same distribution.
\end{definition}

\begin{definition}[Arrival and interarrival times]
For a counting process $N$, for $i \in \mathbb{Z}_{\ge 0}$,
define the $i\Th$ arrival time $S_i \defeq \min_{t \ge 0} (N(t) = i)$.
For $i \in \mathbb{Z}_{\ge 1}$, define the $i\Th$ interarrival time $T_i \defeq S_i - S_{i-1}$.
\end{definition}

\begin{lemma}
For a counting process $N$ with arrival times $S$, $N(t) \ge n \iff S_n \le t$.
\end{lemma}

\section{Poisson Process}

\begin{definition}[Poisson process]
A counting process $N$ is a Poisson process with rate function
$\lambda: \mathbb{R}_{\ge 0} \mapsto \mathbb{R}_{\ge 0}$ if
$N$ has independent increments and $N(t_2)-N(t_1) \sim \Poisson(\mu)$,
where $\mu \defeq \int_{t_1}^{t_2} \lambda(t) dt$.
$N$ is called \emph{homogeneous} if $\lambda(t) = \lambda(0)$ for all $t$,
otherwise it is called \emph{inhomogeneous}.
For a homogeneous process, we denote $\lambda(0)$ by $\lambda$.
\end{definition}

\begin{lemma}
A Poisson process $N$ is homogeneous iff it has stationary increments.
\end{lemma}

\begin{theorem}[Alternative definition of Poisson process]
A counting process $N$ is a Poisson process with continuous rate function $\lambda$ iff
$N$ has independent and stationary increments and
$\Pr(N(t+h)-N(t) = 1) = \lambda(t) h + o(h)$ and $\Pr(N(t+h)-N(t) \ge 2) = o(h)$.
\end{theorem}
\begin{proof}[Proof sketch for homogeneous]
Let $g(u,t) \defeq \MGF_u(N(t)) = \E(e^{uN(t)})$.
Show $g(u,t) = 1 + \lambda t(e^u-1) + o(t)$ straightforwardly.
Use calculus to show that $g(u,t) = \exp(e^{\lambda t}(e^u-1))$
(find derivative w.r.t $t$ by computing $\lim_{h \to 0} (g(u,t+h)-g(u,t))/h$; this gets rid of $o(h)$).
Conclude that $N(t) \sim \Poisson(\lambda t)$ since $g(u,t)$ is MGF of $\Poisson(\lambda t)$.
\end{proof}

\begin{lemma}
For a homogeneous Poisson process $N$,
\[ \Pr(N(s)=a \mid N(s+t)=a+b) = \binom{a+b}{a}\left(\frac{s}{s+t}\right)^a\left(\frac{t}{s+t}\right)^b. \]
\end{lemma}

\begin{theorem}
Let $N$ be a counting process. Then $N$ is a homogeneous Poisson process with rate $\lambda$
iff all interarrival times are independent and distributed $\Expo(\lambda)$.
\end{theorem}

\begin{theorem}[Decomposition theorem 1]
Let $K$ be a finite set, and let $\{N_i: i \in K\}$ be independent Poisson processes,
where $N_i$ has rate function $\lambda_i$. Let $N \defeq \sum_{i \in K}N_i$.
Then $N$ is a Poisson process with rate function $\sum_{i \in K} \lambda_i$.
\end{theorem}

\begin{theorem}[Decomposition theorem 2]
Let $N$ be a Poisson process with rate function $\lambda$. Let $K$ be a finite set (called set of labels).
Suppose the $j\Th$ event receives label $L_j \in K$, where $\Pr(L_j = i) = p_i(S_j)$
for some function $p_i: \mathbb{R}_{\ge 0} \mapsto \mathbb{R}_{\ge 0}$,
and $\{N, L_1, L_2, \ldots\}$ are independent.
For $i \in K$, let $N_i(t)$ be the number of events having label $i$, i.e,
$N_i(t) = \sum_{j=1}^{N(t)} \boolone(L_j = i)$.
Then $N_i$ is a Poisson process with rate function $p_i \lambda$.
Furthermore, all $N_i$ are independent and if all $p_i$ are constant, then
$N_i(t) \mid N(t) \sim \opname{Binom}(N(t), p_i)$.
\end{theorem}

\begin{lemma}
Let $N^{(1)}$ and $N^{(2)}$ be independent homogeneous Poisson processes
with rates $\lambda_1$ and $\lambda_2$. Then
\[ \Pr(S^{(1)}_n < S^{(2)}_m) = \sum_{i=n}^{n+m-1} \binom{n+m-1}{i}
    \frac{\lambda_1^i\lambda_2^{n+m-1-i}}{(\lambda_1 + \lambda_2)^{n+m-1}}. \]
\end{lemma}
\begin{proof}[Proof sketch]
Model as a continuous markov chain with state space $(n_1, n_2)$,
where $n_i$ is the number of events of $N^{(i)}$ that have occurred.
\end{proof}

\begin{theorem}[arrival times distributed as order statistics]
Let $X = [X_1, X_2, \ldots, X_n]$ be IID uniform variables over $[0, t]$.
Let $U = \opname{sorted}(X)$. Let $S = [S_1, S_2, \ldots, S_n]$.
Then conditioned on $N(t) = n$, the distribution of $S$ and $U$ are identical.
\end{theorem}

\end{document}
