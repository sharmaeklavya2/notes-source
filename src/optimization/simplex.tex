\input{header.tex}
\usepackage[shortlabels]{enumitem}
%\usepackage{setspace}

%\onehalfspacing

\newcommand*{\thmdepUrl}{https://sharmaeklavya2.github.io/theoremdep/nodes}
\newcommand*{\todotext}[1]{\textcolor{red}{#1}}
\newenvironment*{tightemize}{\begin{itemize}[noitemsep,topsep=0pt]}{\end{itemize}}
\renewcommand{\algorithmiccomment}[1]{\hfill\textcolor{gray}{\texttt{//} \textit{#1}}}
\algnewcommand{\LineComment}[1]{\State \textcolor{gray}{\texttt{//} \textit{#1}}}

\newcommand*{\Jcomp}{\overline{J}}
\newcommand*{\bline}{\overline{b}}
\newcommand*{\btildline}{\overline{\btild}}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\stdLP}{stdLP}
\DeclareMathOperator{\direction}{direction}
\DeclareMathOperator{\solve}{\mathtt{solve}}
\DeclareMathOperator{\pivot}{\mathtt{pivot}}
\DeclareMathOperator{\assert}{\mathtt{assert}}
\DeclareMathOperator{\simplex}{\mathtt{simplex}}
\DeclareMathOperator{\simplexInit}{\mathtt{simplexInit}}
\DeclareMathOperator{\simplexMove}{\mathtt{simplexMove}}
\DeclareMathOperator{\updateDS}{\mathtt{updateDS}}

\ifdefined\shortVersion
\excludecomment{inLongVersion}
\excludecomment{longProof}
\else
\newenvironment*{inLongVersion}{}{}
\newenvironment*{longProof}{\begin{proof}}{\end{proof}}
\fi

\title{The Simplex Method}

\begin{document}

\maketitle
\setlength{\parskip}{0.4em}

This document describes the \emph{simplex method} for solving linear programs.

\section{Preliminaries}

\begin{theorem}
Any linear programming problem can be reduced to the following problem
(called a \emph{standard form linear program}):
\[ \min_{x \in \mathbb{R}^n}\; c^Tx \textrm{ where } Ax = b \textrm{ and } x \ge 0. \]
Here $A \in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^m$ and $c \in \mathbb{R}^n$.
\end{theorem}

We will also assume \wLoG{} that
$\href{https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/matrices/rank.html}{\rank}(A) = m$.

\begin{inLongVersion}
Read the following concepts at TheoremDep (\url{https://sharmaeklavya2.github.io/theoremdep/}):
\begin{tightemize}
\item \href{\thmdepUrl/convexity/polyhedra/bfs.html}{Basic feasible solution (BFS)}
\item \href{\thmdepUrl/convexity/extreme-point.html}{Extreme point of a convex set}
\item \href{\thmdepUrl/convexity/polyhedra/extreme-point-iff-bfs.html}{Extreme point iff BFS}
\item \href{\thmdepUrl/convexity/polyhedra/orth-lp.html}{LP in orthant is optimized at BFS}
\end{tightemize}
Due to the last point above, we will focus on finding an optimal solution that is also a BFS.
\end{inLongVersion}

\begin{lemma}
\label{thm:replace-vector-in-basis}
Let $B = [u_1, u_2, \ldots, u_n]$ be a basis of a vector space $V$.
Let $w = \sum_{i=1}^n \lambda_i u_i$.
Then $B' = B - \{u_r\} \cup \{w\}$ is a basis of $V$ iff $\lambda_r \neq 0$.
\end{lemma}
\begin{longProof}
(See \url{\thmdepUrl/linear-algebra/vector-spaces/basis/replace-vector.html}.)
\end{longProof}

\begin{inLongVersion}
\begin{lemma}
For any matrix $A$, we have $\rank(A) = \rank(A^T)$.
\end{lemma}
\end{inLongVersion}

\subsection{Notation}

For any non-negative integer $n$, let $[n] \defeq \{1, 2, \ldots, n\}$
(or $[n] \defeq [1, 2, \ldots, n]$, depending on the context).

\begin{inLongVersion}
Let $v \in \mathbb{R}^n$ and $A \in \mathbb{R}^{m \times n}$.
Let $i \in [m]$ and $j \in [n]$.
Then the $j\Th$ element of $v$ is denoted as $v_j$ or $v[j]$.
The element of $A$ in the $i\Th$ row and $j\Th$ column of $A$ is denoted as
$A_{i,j}$ or $A[i,j]$. $A[*,j]$ denotes the $j\Th$ column of $A$ and
$A[i, *]$ denotes the $i\Th$ row of $A$.

Let $J = [j_1, j_2, \ldots, j_r]$ be a sequence of $r$ integers in $[n]$.
$v[J]$ is defined as the vector $[v[j_1], v[j_2], \ldots, v[j_n]]$.
$A[*,J]$ is defined as the matrix whose $k\Th$ column is $A[*,j_k]$.
Let $K = [k_1, k_2, \ldots, k_q]$ be a sequence of $q$ integers in $[m]$.
Then $A[K,*]$ is defined as the matrix whose $i\Th$ column is $A[k_i,*]$.

For matrices $A \in \mathbb{R}^{m \times n_1}$ and $B \in \mathbb{R}^{m \times n_2}$,
let $C = [A, B]$ denote the matrix in $\mathbb{R}^{m \times (n_1 + n_2)}$
where the first $n_1$ columns in $C$ are the columns of $A$
and the last $n_2$ columns in $C$ are the columns of $B$.
We call $C$ the \emph{horizontal concatenation} of $A$ and $B$.
We can similarly define horizontal concatenation of more than two matrices.
We can similarly define vertical concatenation of $A$ and $B$,
which we denote as $\begin{bmatrix}A\\B\end{bmatrix}$.
\end{inLongVersion}

\begin{definition}
Let $\stdLP(A, b, c)$ denote this LP:
\[ \min_{x \ge 0}\; c^Tx \quad\textrm{where}\quad Ax = b. \]
\end{definition}

\section{Bases}

Consider this linear program:
\[ \min_{x \in \mathbb{R}^n}\; c^Tx \textrm{ where } Ax = b \textrm{ and } x \ge 0. \]
Here $A \in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^m$ and $c \in \mathbb{R}^n$.

\begin{definition}[Basis]
Let $J$ be a sequence of $m$ distinct numbers from $[n]$.
Let $B \defeq A[*,J]$.
Then $J$ is called a \emph{basis} of the LP iff $\rank(B) = m$.
$J$ is called a feasible basis iff it is a basis and $B^{-1}b \ge 0$.

Let $\Jcomp$ be the increasing sequence of values of $[n]$ that are not in $J$.
Define $\solve(J)$ as a vector $\xhat \in \mathbb{R}^n$,
where $\xhat[J] = B^{-1}b$ and $\xhat[\Jcomp] = 0$.
\end{definition}

The following two results show that to find an optimal BFS of the LP,
we can find a feasible basis $J$ that minimizes $c^T\solve(J)$, and then return $\solve(J)$.

\begin{lemma}
\label{thm:basis-gives-bfs}
Let $J$ be a feasible basis and $\xhat = \solve(J)$.
Then $\xhat$ is a BFS of the LP.
\end{lemma}
\begin{longProof}
It's trivial to see that $\xhat \ge 0$. Let $B = A[*,J]$ and $N = A[*,\Jcomp]$. Then
\[ A\xhat = B\xhat[J] + N\xhat[\Jcomp] = B(B^{-1}b) = b. \]
Hence, $\xhat$ is feasible for the LP.

Because we can rearrange variables and constraints, we can assume \wLoG{} that $J = [m]$.
The equality constraints are tight, and their coefficient matrix is $A = [B, N]$.
The non-negativity constraints $\{x_j \ge 0: j \in \Jcomp\}$ are tight,
and their coefficient matrix is $I_n[\Jcomp,*] = [0, I_{n-m}]$,
where $I_k$ denotes the $k$-by-$k$ identity matrix.
Hence, the rank of the coefficient matrix of tight constraints at $\xhat$ is
\[ \rank\left(\begin{bmatrix}B & N \\ 0 & I_{n-m}\end{bmatrix}\right)
= \rank\left(\begin{bmatrix}B & 0 \\ 0 & I_{n-m}\end{bmatrix}\right)
= \rank(B) + (n-m) = n. \]
The first equation follows from the fact that rank is unaffected by row operations.
The third equation follows from the fact that $J$ is a basis.
Since the coefficient matrix of tight constraints of $\xhat$ has rank $n$,
$\xhat$ is a BFS of the LP.
\end{longProof}

\begin{lemma}
\label{thm:bfs-gives-basis}
Let $\xhat$ be a BFS of the LP. Then there exists a feasible basis $J$
such that $\xhat = \solve(J)$.
\end{lemma}
\begin{longProof}
Since $\xhat$ is a BFS, there exist $n$ linearly independent constraints that are tight at $\xhat$.
$m$ of these are the equality constraints, whose coefficient matrix is $A$.
The rest are inequality constraints.
Let $\Jcomp$ be the indices of these $n-m$ inequality constraints.
This implies $\xhat[\Jcomp] = 0$.
Since we can rearrange variables, assume \wLoG{} that $\Jcomp = [m+1, m+2, \ldots, n]$.
The coefficient matrix of these constraints is $I_n[\Jcomp,*] = [0, I_{n-m}]$.

Let $J = [m]$. Let $B = A[*,J]$ and $N = A[*,J]$. Then $A = [B, N]$. Since $\xhat$ is a BFS, we get
\[ n = \rank\left(\begin{bmatrix}B & N \\ 0 & I_{n-m}\end{bmatrix}\right)
= \rank\left(\begin{bmatrix}B & 0 \\ 0 & I_{n-m}\end{bmatrix}\right)
= \rank(B) + (n-m). \]
This implies that $\rank(B) = m$, which shows that $J$ is a basis of the LP.

Furthermore, since $\xhat$ is feasible for the LP, we get that
$b = A\xhat = B\xhat[J] + N\xhat[\Jcomp] = B\xhat[J]$.
Hence, $\xhat[J] = B^{-1}b$. Since $\xhat$ is feasible for the LP,
we get $\xhat \ge 0 \implies \xhat[J] \ge 0 \implies B^{-1}b \ge 0$.
Hence, $J$ is a feasible basis and $\solve(J) = \xhat$.
\end{longProof}

\section{The Simplex Algorithm}

See \cref{algo:simplex} for the description of the simplex algorithm.
The input to the algorithm is $(A, b, c, J)$, where $J$ is a feasible basis of $Ax = b$.
The algorithm initializes a data structure $D$ using $J$
(by calling the subroutine $\simplexInit$),
and then iteratively updates $J$ and the data structure $D$
(by calling subroutines $\simplexMove$ and $\updateDS$).
Specifically, if the \texttt{status} output by $\simplexMove$ is \texttt{move},
then it outputs a pair $(k, r)$ of integers, where $k \in [n] - J$ and $r \in [m]$.
It then sets the $r\Th$ element of $J$ to $k$.
We say that $J[r]$ \emph{leaves the basis} and $k$ \emph{enters the basis}.

\begin{algorithm}[H]
\caption{$\simplex(A, b, c, J)$:
$A \in \mathbb{R}^{m \times n}$, $b \in \mathbb{R}^m$, $c \in \mathbb{R}^n$,
and $J$ is a feasible basis for $\stdLP(A, b, c)$.
}
\begin{algorithmic}[1]
\LineComment{contains some \href{https://docs.python.org/3/reference/simple\_stmts.html\#assignment-statements}{Python assignment syntax}}
\State $D = \simplexInit(A, b, c, J)$
\While{true}
    \State \texttt{status, *outs} = $\simplexMove(D, J)$
    \LineComment{\texttt{status} can be \texttt{optimal}, \texttt{unbounded}, or \texttt{move}.}
    \LineComment{\texttt{outs} is a list}
    \If{\texttt{status == move}}
        \State $(k, r, \delta)$ = \texttt{outs}
        \State\label{alg-line:simplex:J}$J[r] = k$
        \State $D = \updateDS(D, J, k, r)$
    \Else
        \State \Return $(\texttt{status}, J, \texttt{*outs})$
    \EndIf
\EndWhile
\end{algorithmic}
\label{algo:simplex}
\end{algorithm}

There are different variants of the simplex algorithm, depending on what
data structure $D$ they maintain.
We will look at 3 variants: naive simplex, tableau simplex, and revised simplex.
In the \emph{naive simplex method}, we set $D \defeq (A, b, c)$.
Hence, $\simplexInit$ and $\updateDS$ are trivial for naive simplex.
The main advantage of tableau and revised over naive is that they speed up $\simplexMove$.

\begin{definition}
Let $J \defeq [j_1, \ldots, j_m]$ be a basis of $\stdLP(A, b, c)$,
where $A \in \mathbb{R}^{m \times n}$, and let $k \in [n] - J$.
Let $B \defeq A[*,J]$ and $Y \defeq B^{-1}A$.
Then define $\direction(J, k) \in \mathbb{R}^n$ as the vector $y$ where
\[ y_t = \begin{cases}
    -Y[i, k] & \textrm{ if } t = j_i
    \\ 1 & \textrm{ if } t = k
    \\ 0 & \textrm{ otherwise}
    \end{cases}. \]
\end{definition}

The core of the simplex algorithm is $\simplexMove$,
which tells us how to move from one basis to another.
$\simplexMove$ is described in \cref{algo:simplexMove}.
Specifically, when $\simplexMove(D, J)$ outputs $(\mathtt{move}, k, r, \delta)$,
it moves from $\solve(J)$ to $\solve(J) + \delta\direction(J, k)$ (we will prove this soon).

\begin{algorithm}[H]
\caption{$\simplexMove(D, J)$:
$J$ is a feasible basis of $\stdLP(A, b, c)$.}
\begin{algorithmic}[1]
\State Let $B \defeq A[*, J]$, $Y \defeq B^{-1}A$, $\bline \defeq B^{-1}b$, and $z = Y^Tc[J]$.
\LineComment{We will lazily compute $B$, $Y$, $\bline$, and $z$ using $D$.}
\If{$c - z \ge 0$}
    \State\label{alg-line:simplexMove:opt}\Return $(\mathtt{optimal}, \solve(J), c[J]^T\bline)$
\EndIf
\State Find $k \in [n]$ such that $c_k - z_k < 0$.
\If{$Y[*, k] \le 0$}\label{alg-line:simplexMove:Y-neg}
    \State\label{alg-line:simplexMove:unb}%
\Return $(\mathtt{unbounded}, \solve(J), \direction(J, k), k)$
\EndIf
\State $\displaystyle r = \argmin_{i \in [m]: Y[i, k] > 0} \frac{\bline_i}{Y[i, k]}$
\State $\delta = \bline_r/Y[r, k]$
\State \Return $(\mathtt{move}, k, r, \delta)$.
\end{algorithmic}
\label{algo:simplexMove}
\end{algorithm}

Since $\simplexMove$ requires $J$ to be a feasible basis of $\stdLP(A, b, c)$,
and we're changing $J$ in line \ref{alg-line:simplex:J}, we need to prove
that after this change, $J$ continues to be a feasible basis of $\stdLP(A, b, c)$.

\begin{theorem}
\label{thm:simplex-optimal}
If $\simplex$ outputs $(\mathtt{optimal}, J, \xhat, \beta)$,
then $\xhat$ is a BFS of the LP and an optimal solution to the LP.
Furthermore, $\xhat = \solve(J)$ and $\beta = c^T\xhat$.
\end{theorem}
\begin{proof}[Proof sketch]
For any feasible $x$, we can show that $c^Tx = c[J]^T\bline + (c-z)[\Jcomp]^Tx[\Jcomp]$.
Since $c[J]^T\bline = c^T\xhat$, $x[\Jcomp] \ge 0$, and $c - z \ge 0$,
we get $c^Tx \ge c^T\xhat$.
\end{proof}
\begin{longProof}
By line \ref{alg-line:simplexMove:opt} of $\simplexMove$,
$\xhat = \solve(J)$ and $\beta = c[J]^T\bline$.
Hence, $\xhat$ is a BFS by \cref{thm:basis-gives-bfs} and $c^T\xhat = \beta$.
Now we just need to prove that $\xhat$ is optimal.

Let $\Jcomp = [n] - J$. Let $N = A[*,\Jcomp]$.
Let $x_B = x[J]$ and $x_N = x[\Jcomp]$. Then
\[ Ax = b \iff Bx_B + Nx_N = b \iff x_B = \bline - B^{-1}Nx_N. \]
Note that since the constraint $x_B = \bline - B^{-1}Nx_N$ is equivalent to $Ax = b$,
we can replace $Ax = b$ by $x_B = \bline - B^{-1}Nx_N$ in the LP without affecting
the set of feasible solutions.

We can use these new constraints to express the objective value as a function of $x_N$.
\begin{align*}
c^Tx &= c[J]^Tx_B + c[\Jcomp]^Tx_N
\\ &= c[J]^T\left(\bline - B^{-1}Nx_N\right) + c[\Jcomp]^Tx_N
\\ &= c[J]^T\bline + (c[\Jcomp]^T - c[J]^TB^{-1}N)x_N
\end{align*}
\[ z[\Jcomp]^T = (c[J]^TY)[\Jcomp] = c[J]^TB^{-1}A[*,\Jcomp] = c[J]^TB^{-1}N. \]
\[ \implies c^Tx = c[J]^T\bline + (c-z)[\Jcomp]^Tx_N. \]
From the non-negativity constraints, we know that $x_N \ge 0$.
We also know that $c-z \ge 0$, since $\simplexMove$'s output status is \texttt{optimal}.
Hence, for every feasible $x$, we have
$c^Tx = c[J]^T\bline + (c-z)[\Jcomp]^Tx_N \ge c[J]^T\bline = c^T\xhat$.
Hence, $\xhat$ is an optimal solution to the LP.
\end{longProof}

\begin{lemma}
\label{thm:zJ-eq-cJ}
$z[J] = c[J]$.
\end{lemma}
\begin{proof}
$z[J]^T = c[J]^T(B^{-1}A)[*,J] = c[J]^TB^{-1}A[*,J] = c[J]^T$.
\end{proof}
\Cref{thm:zJ-eq-cJ} implies that $k \not\in J$, since $c_k - z_k < 0$.

\begin{lemma}
\label{thm:YJ-is-I}
$Y[*, J] = I$.
Let $J = [j_1, j_2, \ldots, j_m]$.
Then $Y[i, j_p] = \begin{cases}1 & \textrm{ if } p = i
\\ 0 & \textrm{ if } p \neq i\end{cases}$.
\end{lemma}
\begin{longProof}
\[ Y[*, J] = (B^{-1}A)[*, J] = B^{-1}A[*, J] = B^{-1}B = I. \]
\[ Y[i, j_p] = Y[*, J][i, p] = I[i, p] = \begin{cases}1 & \textrm{ if } p = i
\\ 0 & \textrm{ if } p \neq i\end{cases}. \qedhere \]
\end{longProof}

\begin{lemma}
\label{thm:y-in-nullsp}
Let $y = \direction(J, k)$. Then $Yy = Ay = 0$.
\end{lemma}
\begin{longProof}
\begin{align*}
(Yy)_i &= \sum_{j=1}^n Y[i, j]y_j
= \sum_{p=1}^m Y[i, j_p]y_{j_p} + Y[i, k]y_k
\\ &= y_{j_i} + Y[i, k]y_k
= -Y[i, k] + Y[i, k] = 0.
\end{align*}
\[ Ay = B^{-1}Yy = B^{-1}0 = 0. \qedhere \]
\end{longProof}

\begin{lemma}
\label{thm:y-reduces-cost}
Let $y \defeq \direction(J, k)$. Then $c^Ty = c_k - z_k$.
\end{lemma}
\begin{longProof}
\begin{align*}
c^Ty &= \sum_{j=1}^n c_jy_j = c_ky_k + \sum_{p=1}^m c_{j_p}y_{j_p}
= c_k - \sum_{p=1}^m c_{j_p}Y[p, k]
\\ &= c_k - \sum_{p=1}^m Y^T[k, p]c[J]_p
= c_k - (Y^Tc[J])_k = c_k - z_k < 0.
\qedhere \end{align*}
\end{longProof}

\begin{theorem}
\label{thm:simplex-unbounded}
If $\simplex$ outputs $(\textrm{unbounded}, J, \xhat, y, k)$,
then the LP's cost reduces along the ray $\{\xhat + \lambda y: \lambda \ge 0\}$
and the ray is feasible, which implies that the LP is unbounded.
Furthermore, $y \ge 0$, $\xhat = \solve(J)$, and $y = \direction(J, k)$.
\end{theorem}
\begin{longProof}
By line \cref{alg-line:simplexMove:unb} of $\simplexMove$,
we know that $\xhat = \solve(J)$ and $y = \direction(J, k)$.

By \cref{thm:y-in-nullsp}, we know that $Ay = 0$.
Hence, $A(\xhat + \lambda y) = A\xhat = b$.
Since $\simplexMove$ returned $(\textrm{unbounded}, \xhat, y, k)$,
we get that $Y[*,k] \le 0$ (by \cref{alg-line:simplexMove:Y-neg}).
Hence, $y \ge 0$ and so $\xhat + \lambda y \ge \xhat \ge 0$.
Hence, $\xhat + \lambda y$ is feasible for the LP for all $\lambda \ge 0$.

By \cref{thm:y-reduces-cost}, we know that $c^Ty = c_k - z_k < 0$,
Hence, moving along the ray will reduce cost indefinitely.
This implies that the LP is unbounded.
\end{longProof}

\begin{lemma}
\label{thm:simplex-new-basis}
Suppose $\simplexMove(D, J)$ outputs $(\mathtt{move}, k, r, \delta)$.
Let $\Jtild$ be the new sequence obtained by changing $J[r]$ to $k$
(at line \ref{alg-line:simplex:J} of $\simplex$).
Then $\Jtild$ is a basis of the LP.
\end{lemma}
\begin{proof}
Let $J = [j_1, j_2, \ldots, j_m]$.
The set of values in $\Jtild$ is $J - \{j_r\} \cup \{k\}$.
Since $k \not\in J$, $\Jtild$ has distinct values.

Let $a_j$ be the $j\Th$ column of $A$. Let $B = A[*,J]$. Let $\Btild = A[*,\Jtild]$.
Let $S = \{a_{j_1}, a_{j_2}, \ldots, a_{j_m}\}$ be the set of columns of $B$
and let $\Stild = S - \{a_{j_r}\} \cup \{a_k\}$ be the set of columns of $\Btild$.
Since $J$ is a basis, $\rank(B) = m$, so $S$ is a set of linearly independent vectors.
Since $|S| = m$, we get that $S$ is a basis of $\mathbb{R}^m$. Hence, $a_k \in \Span(S)$.

Let $a_k = \sum_{i=1}^m \lambda_i a_{j_i}$. Let $\lambda = [\lambda_1, \lambda_2, \ldots, \lambda_m]$.
Then $B\lambda = \sum_{i=1}^m \lambda_i a_{j_i} = a_k$.
Hence, $\lambda = B^{-1}a_k = Y[*,k]$.
Since $Y[r,k] > 0$, we get that $\lambda_r > 0$.
Hence, by \cref{thm:replace-vector-in-basis}, we get that
$\Stild$ is also a basis of $\mathbb{R}^m$.
Hence, $\rank(\Btild) = m$, so $\Jtild$ is a basis.
\end{proof}

\begin{lemma}
\label{thm:simplex-feasible-basis}
Suppose $\simplexMove(D, J)$ outputs $(\mathtt{move}, k, r, \delta)$.
Let $\Jtild$ be the new sequence obtained by changing $J[r]$ to $k$
(at line \ref{alg-line:simplex:J} of $\simplex$).
Then $\Jtild$ is a feasible basis of the LP.
Furthermore, let $y = \direction(J, k)$, $\xhat = \solve(J)$, and $\xtild = \xhat + \delta y$.
Then $\xtild = \solve(\Jtild)$ and $c^T\xtild \le c^T\xhat$.
\end{lemma}
\begin{proof}[Proof sketch]
We can show that $A\xtild = b$, $\xtild \ge 0$, and $\xtild_j = 0$ when $j \not\in \Jtild$.
Let $\Btild \defeq A[*,\Jtild]$. Then
$b = A\xtild = A[*,\Jtild]\xtild[\Jtild] = \Btild\xtild[\Jtild]$.
So, $\xtild[\Jtild] = \Btild^{-1}b$, which implies $\xtild = \solve(\Jtild)$.
Also, $c^T(\xtild - \xhat) = \delta(c^Ty) = \delta(c_k - z_k) \le 0$ by \cref{thm:y-reduces-cost}.
\end{proof}
\begin{longProof}
By \cref{thm:y-in-nullsp}, we get that $Ay = 0$.
Hence, $A\xtild = A\xhat + \delta (Ay) = A\xhat = b$.

If $i \not\in J$ or $Y[i, k] \le 0$, then $y_i \ge 0$,
and hence $\xtild_i = \xhat_i + \delta y_i \ge \xhat_i \ge 0$.
Now let $i \in J$ and $Y[i,k] > 0$.
Let $J = [j_1, j_2, \ldots, j_m]$. Then
\[ \delta = \frac{\bline_r}{Y[r,k]} \le \frac{\bline_i}{Y[i, k]}. \]
\[ \implies \xtild_{j_i} = \xhat_{j_i} + \delta y_{j_i} = \bline_i - \delta Y[i,k] \ge 0. \]
Hence, $\xtild \ge 0$. Therefore, $\xtild$ is feasible for the LP.

Let $i \in [n] - \Jtild$. If $i = j_r$, then
\[ \xtild_i = \xhat_{j_r} + \delta y_{j_r} = \bline_r - \delta Y[r,k] = 0. \]
If $i \in [n] - J - \{k\}$, then $\xtild_i = \xhat_i + \delta y_i = 0 + \delta 0 = 0$.
Hence, $\xtild_i = 0$ when $i \not\in \Jtild$.
Let $\Btild \defeq A[*,\Jtild]$. Then
\[ b = A\xtild = A[*,\Jtild]\xtild[\Jtild] = \Btild\xtild[\Jtild]. \]
By \cref{thm:simplex-new-basis}, $\Jtild$ is a basis, so $\Btild$ is invertible.
Hence, $\xtild[\Jtild] = \Btild^{-1}b$.
Furthermore, $\xtild[[n] - \Jtild] = 0$, so $\xtild = \solve(\Jtild)$.
Since $\xtild \ge 0$, we get that $\Btild^{-1}b \ge 0$.
Hence, $\Jtild$ is a feasible basis.

Also, $c^T(\xtild - \xhat) = \delta(c^Ty) = \delta(c_k - z_k) \le 0$ by \cref{thm:y-reduces-cost}.
Hence, $c^T\xtild \le c^T\xhat$.
\end{longProof}

This completes the correctness of $\simplex$.

\section{Implementations of Simplex}

The naive simplex method has a large running time of $O(m^2(m + n))$ per iteration,
since we compute $B^{-1}$, $Y$, $\bline$ and $z$ afresh in each iteration.
We will now see how the tableau method and the revised simplex method
improve the running time per iteration.

In the Tableau method, the data structure $D$ is
\[ \begin{bmatrix} c - z & -c[J]^T\bline \\[0.5em] Y & \bline \end{bmatrix}, \]
where the rows are numbered from 0 instead of 1.
In the Revised simplex method, the data structure $D$ is given by
the pair $(D_1, D_2)$, where $D_1 \defeq (A, b, c)$ and
\[ D_2 \defeq \begin{bmatrix} -c[J]^TB^{-1} & -c[J]^T\bline \\ B^{-1} & \bline \end{bmatrix}, \]
where the rows are numbered from 0 instead of 1.
It is easy to see that we can quickly compute $Y$, $\bline$, and $c-z$
in $\simplexMove$ in both methods.
$\simplexInit$ is implemented in the obvious straightforward way.
We will now see how to implement $\updateDS$ using elementary row operations.

\begin{definition}[pivoting]
Let $A \in \mathbb{R}^{m \times n}$ be a matrix, $i \in [m]$, and $j \in [n]$
such that $A[i, j] \neq 0$. Then pivoting is the operation of applying
elementary row operations to $A$ to get a new matrix $\Ahat \in \mathbb{R}^{m \times n}$
such that $\Ahat[i, j] = 1$ and $\Ahat[i', j] = 0$ for all $i' \in [m] - \{i\}$.
\end{definition}

In the tableau method, $\updateDS(D, J, k, r)$ is performed by pivoting $D$ at $(r, k)$.
In the revised simplex method, $\updateDS(D, J, k, r)$ is performed by
horizontally concatenating the column
$\begin{bmatrix}c_k - z_k \\ Y[*, k] \end{bmatrix}$ to $D_2$,
(which becomes the $(m+2)\Th$ column), pivoting at $(r, m+2)$,
and then discarding the $(m+2)\Th$ column.

Let $J$ be a feasible basis of the LP.
Let $B \defeq A[*,J]$, $Y \defeq B^{-1}A$, $\bline \defeq B^{-1}b$ and $z \defeq Y^Tc[J]$.
Based on how $k$ and $r$ are chosen, we know that $c_k - z_k < 0$,
$Y[r,k] > 0$, and $r \in \argmin_{i \in [m]: Y[i,k] > 0} \frac{\bline_i}{Y[i,k]}$.
Let $\Jtild$ be the sequence obtained by changing the $r\Th$ element of $J$ to $k$.
By \cref{thm:simplex-feasible-basis}, $\Jtild$ is a feasible basis.
Let $\Btild \defeq A[*,\Jtild]$, $\Ytild \defeq \Btild^{-1}A$, $\btildline \defeq \Btild^{-1}b$
and $\ztild \defeq \Ytild^Tc[\Jtild]$.
We will now see how to compute $\Ytild$, $\ztild$ and $\btildline$
from $Y$, $z$ and $\bline$.

Define the matrix $\Yhat$ as
\[ \Yhat[i,j] = \begin{cases}
\displaystyle \frac{Y[r,j]}{Y[r,k]} & \textrm{ if } i = r
\\[1em] \displaystyle Y[i,j] - \frac{Y[i,k]}{Y[r,k]}Y[r,j] & \textrm{ if } i \neq r
\end{cases}. \]
Note that $\Yhat$ is obtained from $Y$ by pivoting on $(r, k)$.
Let $R$ be the matrix of these row operations. Then $\Yhat = RY$.
We can find $R$ by applying these row operations to the $m$-by-$m$ identity matrix.
\begin{align*}
R[i,j] &= \begin{cases}
\displaystyle \frac{I[r,j]}{Y[r,k]} & \textrm{ if } i = r
\\[1em] \displaystyle I[i,j] - \frac{Y[i,k]}{Y[r,k]}I[r,j] & \textrm{ if } i \neq r
\end{cases}
\\ &= \begin{cases}
\displaystyle \frac{1}{Y[r,k]} & \textrm{ if } i = r = j
\\[1em] \displaystyle -\frac{Y[i,k]}{Y[r,k]} & \textrm{ if } i \neq r \land j = r
\\ \displaystyle 1 & \textrm{ if } i \neq r \land j = i
\\ \displaystyle 0 & \textrm{ if } j \not\in \{i, r\}
\end{cases}.
\end{align*}

\begin{lemma}
\label{thm:upd-Y}
$\Btild^{-1} = RB^{-1}$ and $\Ytild = RY$ and $\btildline = R\bline$.
\end{lemma}
\begin{longProof}
Let $J = [j_1, j_2, \ldots, j_m]$. $\Jtild = J - \{j_r\} \cup \{k\}$.
By \cref{thm:YJ-is-I}, we get that $Y[*,J] = \Ytild[*,\Jtild] = I$.
We will try to show that $\Yhat[*,\Jtild] = I$.

Let $p, q \in [m] - \{r\}$.
\[ \Yhat[*,\Jtild][r,r] = \Yhat[r,\Jtild[r]] = \Yhat[r,k] = 1. \]
\[ \Yhat[*,\Jtild][r,q] = \Yhat[r,\Jtild[q]] = \Yhat[r,j_q] = \frac{Y[r,j_q]}{Y[r,k]} = 0.
    \tag{by \cref{thm:YJ-is-I}} \]
\[ \Yhat[*,\Jtild][p,r] = \Yhat[p,\Jtild[r]] = \Yhat[p,k]
    = Y[p,k] - \frac{Y[p,k]}{Y[r,k]}Y[r,k] = 0. \]
\begin{align*}
\Yhat[*,\Jtild][p,q] &= \Yhat[p,\Jtild[q]] = \Yhat[p,j_q]
= Y[p,j_q] - \frac{Y[p,k]}{Y[r,k]}Y[r,j_q] = Y[p,j_q]
\\ &= \begin{cases}1 & \textrm{ if } p = q \\ 0 & \textrm{ otherwise}\end{cases}.
\tag{by \cref{thm:YJ-is-I}}
\end{align*}
Hence, $\Yhat[*,\Jtild] = I$.

\[ I = \Yhat[*,\Jtild] = (RB^{-1}A)[*,\Jtild] = RB^{-1}A[*,\Jtild] = RB^{-1}\Btild. \]
Hence, $\Btild^{-1} = RB^{-1}$.
\[ \Ytild = \Btild^{-1}A = RB^{-1}A = RY. \]
\[ \btildline = \Btild^{-1}b = RB^{-1}b = R\bline. \qedhere \]
\end{longProof}

Define $\zhat \in \mathbb{R}^n$ and $\eta$ as
\begin{align*}
\zhat_j &= z_j + \frac{c_k - z_k}{Y[r,k]} Y[r,j]
& \eta &= c[J]^T\bline + \frac{c_k - z_k}{Y[r, k]}\bline_r.
\end{align*}

\begin{lemma}
\label{thm:upd-z}
$\zhat = \ztild$ and $\eta = c[\Jtild]^T\btildline$.
\end{lemma}
\begin{longProof}
Let $J = [j_1, j_2, \ldots, j_m]$. Then $\Jtild = J - \{j_r\} \cup \{k\}$.
Let $i \in [m] - \{r\}$. Then
\[ \zhat[\Jtild]_i = \zhat_{j_i} = z_{j_i} + \frac{c_k - z_k}{Y[r,k]} Y[r,j_i] = z_{j_i}. \]
By \cref{thm:YJ-is-I}, we get $Y[r,j_i] = 0$.
By \cref{thm:zJ-eq-cJ}, we get $z_{j_i} = c_{j_i}$.
Hence, $\zhat[\Jtild]_i = c_{j_i} = c[\Jtild]_i$.
\[ \zhat[\Jtild]_r = \zhat_k = z_k + \frac{c_k - z_k}{Y[r,k]} Y[r,k] = c_k = c[\Jtild]_r. \]
Hence, $\zhat[\Jtild] = c[\Jtild]$.

\[ Y[r,*] = (B^{-1}A)[r,*] = B^{-1}[r,*]A. \]
\[ \bline_r = (B^{-1}b)_r = B^{-1}[r,*]b. \]
Let $\alpha = (c_k - z_k)/Y[r,k]$. Then
\[ \zhat^T = z^T + \alpha Y[r,*] = c[J]^TB^{-1}A + \alpha B^{-1}[r,*]A. \]
\[ \eta = c[J]^T\bline + \alpha \bline_r = c[J]^TB^{-1}b + \alpha B^{-1}[r,*]b. \]
Let $u^T = c[J]^TB^{-1} + \alpha B^{-1}[r,*]$. Then $\zhat^T = u^TA$ and $\eta = u^Tb$.
\[ c[\Jtild]^T = \zhat[\Jtild]^T = (u^TA)[\Jtild] = u^TA[*,\Jtild] = u^T\Btild. \]
Hence, $u^T = c[\Jtild]^T\Btild^{-1}$.
So, $\zhat = c[\Jtild]^T\Btild^{-1}A = c[\Jtild]^T\Ytild = \ztild$
and $\eta = c[\Jtild]^T\Btild^{-1}b = c[\Jtild]^T\btildline$.
\end{longProof}

In the revised simplex method, we can obtain further speedup in $\simplexMove$.
Compute $c[J]^TB^{-1}$ by multiplying $c[J]^T$ and $B^{-1}$.
Then we iterate over $j \in [n] - \Jtild$, and compute $z_j = (c[J]^TB^{-1})A[*,j]$.
We stop iterating when we find a suitable $k \in [n] - \Jtild$ such that $c_k - z_k < 0$,
or if $c_j - z_j \ge 0$ for all $j \in [n] - \Jtild$.
Next, we compute $u = B^{-1}A[*,k]$ and $\bline = B^{-1}b$.
At the end of the iteration, we can update $B^{-1}$ using row operations as per \cref{thm:upd-Y}.
This is possible since $R$ is defined by $u$.

The time taken is $O(m(t+m))$, where $t$ is the number of variables
that need to be considered till we find $k$. Note that $t \le n-m$.
The space complexity of revised simplex (in addition to storing the input) is $O(m^2)$.

\end{document}
